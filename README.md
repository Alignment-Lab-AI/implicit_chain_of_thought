# Implicit Chain of Thought Reasoning via Knowledge Distillation

Here we provide code to reproduce our results.

## Prerequisites

* [Pytorch](https://pytorch.org/get-started/locally/)

We also need a custom version of Hugging Face's transformers library.

```
git clone https://github.com/da03/implicit_transformers.git
cd implicit_transformers
pip install --editable .
```

## Datasets & Pretrained Models & Logs

* 4 X 4 Mult: [data]() [model]() [log]()
* 5 X 5 Mult: [data]() [model]() [log]()
* GSM8K-Aug: [data]() [model]() [log]()

## Usage

We use 4 X 4 Mult as an example.

### Data Preprocessing


### Training



### Generation & Evaluation



```
```

## Multiple Reasoning Pathways (GSM8K-Aug)

### Training


## Citation

```
@inproceedings{
    anonymous2023implicit,
    title={Implicit Chain of Thought Reasoning via Knowledge Distillation},
    author={Anonymous},
    booktitle={Submitted to The Twelfth International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=9cumTvvlHG},
    note={under review}
}
```
